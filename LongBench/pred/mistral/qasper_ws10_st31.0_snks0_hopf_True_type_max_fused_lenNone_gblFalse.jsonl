{"pred": "The ground truth for fake news is not claimed to be perfect in the article, as it is based on human annotation.", "answers": ["Ground truth is not established in the paper"], "all_classes": null, "length": 3141}
{"pred": "The GhostVLAD approach is an extension of the NetVLAD approach for pooling features in deep learning models, which adds ghost clusters to improve robustness to outliers.", "answers": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "all_classes": null, "length": 2454}
{"pred": "The model outperforms the state of the art results by 6.8% to 12.8%.", "answers": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "all_classes": null, "length": 3207}
{"pred": "Text from context tweets is proposed as an additional feature for neural network models.", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "They chose different pages, including Fox News, CNN, ESPN, and others.", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3735}
{"pred": "The evaluation protocol and baseline are provided for concept-level importance estimation.", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": "The model was evaluated on three summarization datasets: CNN, DailyMail, and XSum.", "answers": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "all_classes": null, "length": 4369}
{"pred": "The proposed approach achieves better correlation with human judgments on word similarity datasets compared to other WSD approaches using word embeddings, as shown in Table TABREF{TABREF{TABREF{TABREF{TABREF{TABREF{TABREF{TABREF{TABREF{TABREF{TABREF{TABREF{TABREF{TABREF{TABREF{TABREF{TABREF{TABREF{TABREF{TABREF{TABREF{TABREF{TABREF{TABREF{T", "answers": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "all_classes": null, "length": 2189}
{"pred": "The ensemble is formed by averaging the predictions of multiple models. The models are selected based on their performance in the validation set.", "answers": ["simply averaging the predictions from the constituent single models"], "all_classes": null, "length": 4212}
{"pred": "The datasets come from the scripts of the Friends TV show (Friends) and Facebook chats (Emotion).", "answers": ["Friends TV sitcom, Facebook messenger chats"], "all_classes": null, "length": 3185}
{"pred": "English.", "answers": ["English", "Simple English"], "all_classes": null, "length": 2243}
{"pred": "The IMDb dataset of movie reviews is used for sentiment analysis.", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "The article does not provide the accuracy of the proposed system.", "answers": ["F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ", "F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)"], "all_classes": null, "length": 5310}
{"pred": "Yes.", "answers": ["No"], "all_classes": null, "length": 3445}
{"pred": "The set of tweets has been crawled and the set of news articles has been extracted.", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": "The article does not provide information on which sector achieved the best performance.", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": "They compared the basic RNN-based NMT model, targeting word-level alignment, with several variants, including targeting character-level alignment, using attention mechanism, and using pre-trained word embeddings. They also compared it with a statistical machine translation (SMT) system.", "answers": ["RNN-based NMT model, Transformer-NMT"], "all_classes": null, "length": 3708}
{"pred": "The three regularization terms are: (1) a regularization term associated with neutral features, (2) the maximum entropy regularization term, and (3) the KL divergence regularization term.", "answers": ["a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution", "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution"], "all_classes": null, "length": 3604}
{"pred": "The baselines are SVM with n-grams, Naive Bayes, Logistic Regression, and Random Forest.", "answers": ["SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information", "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information"], "all_classes": null, "length": 4512}
{"pred": "The article does not provide the exact improvement in performance.", "answers": ["They decrease MAE in 0.34"], "all_classes": null, "length": 2735}
{"pred": "Their model allows for the identification of specific attention heads contributing to the output, providing a clearer understanding of the model's focus.", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": "The baseline was a context-free sentence-level machine translation model.", "answers": [" MT system on the data released by BIBREF11", "Transformer base, two-pass CADec model"], "all_classes": null, "length": 3716}
{"pred": "The evaluation is carried out on two zero-shot cross-lingual tasks: Cross-lingual Natural Language Inference (XNLI) and Cross-lingual Dependency Parsing (XDEP). The metrics used for evaluation are accuracy for XNLI and F1 score for XDEP.", "answers": ["translation probabilities, Labeled Attachment Scores (LAS)", "accuracy, Labeled Attachment Scores (LAS)"], "all_classes": null, "length": 3402}
{"pred": "The attention module is pretrained on ASR and MT tasks.", "answers": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "all_classes": null, "length": 4656}
{"pred": "Stylistic features include patterns related to situational context, such as sarcasm, irony, and humor, as well as linguistic features like tone, rhythm, and word choice. These features are used to detect sarcasm and other forms of figurative language in text.", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "The encoder is an LSTM (Long Short-Term Memory) network.", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "Yes, WordNet is a lexical database for English language words, and it is particularly useful for taxonomic reasoning, including hypernymy (ISA) and hyponymy relationships. The article describes the use of WordNet for probing taxonomic reasoning abilities of models in the context of multiple-choice questions.", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 6391}
{"pred": "The article reports SOTA results on LibriSpeech test-clean set and competitive results on Wall Street Journal test set using Jasper architecture with batch normalization and residual connections. The authors also compare their results to wav2letter and Transformer models.", "answers": ["Unanswerable", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "all_classes": null, "length": 1856}
{"pred": "The dataset consists of 22,500 users.", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "The article mentions using BLEU, perplexity, and human evaluation for evaluation.", "answers": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "all_classes": null, "length": 2673}
{"pred": "They label conversation turns with open-domain inquiries and responses, and also label the clinical entities mentioned in the conversation.", "answers": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "all_classes": null, "length": 3424}
{"pred": "The article does not provide information on how much data is needed to train the task-specific encoder.", "answers": ["57,505 sentences", "57,505 sentences"], "all_classes": null, "length": 4371}
{"pred": "The models are evaluated on four machine translation tasks: IWSLT German-English, WMT English-German, WMT English-French, and WMT English-Romanian.", "answers": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "all_classes": null, "length": 4898}
{"pred": "The article reports significant improvements for Estonian ELMo (ELMo for Estonian) in the NER task compared to fastText baselines.", "answers": ["5 percent points.", "0.05 F1"], "all_classes": null, "length": 3290}
{"pred": "They are scholars from diverse disciplines.", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": "Yes.", "answers": ["No", "No"], "all_classes": null, "length": 2239}
{"pred": "The Nguni languages (zulu, xhosa, ndebele, swati) are similar to each other. The Sotho languages (Sesotho, Sesotho sa Leboa, Sesotho se Seng) are similar to each other. The Nguni and Sotho languages are harder to distinguish.", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "They compared 6-layer and 8-layer LSTM models with Xavier initialization and sequence-to-sequence models with softmax and sequence-to-sequence models with sotp loss. They also compared models trained with CE loss and models trained with sotp loss.", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": "The article describes two datasets: one consisting of around 29.8 million Wikipedia articles and another consisting of three subsets of arXiv academic papers, each with around 110,000-120,000 documents.", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": "The human evaluations were conducted by a group of 50 native speakers who rated the translations on a scale of 1 to 5 for adequacy, fluency, and overall quality.", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": "Yes, they test their framework on English-to-German language pair using TED corpus.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4472}
{"pred": "The efficiency of communication schemes is quantified by the retention rate of keywords and the accuracy of the system's decoding of the user's intended message.", "answers": ["by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews", "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence"], "all_classes": null, "length": 1873}
{"pred": "Precision, recall, F1 score, and accuracy are commonly used evaluation metrics for classification tasks.", "answers": ["Precision, Recall, F-measure, accuracy", "Precision, Recall and F-measure"], "all_classes": null, "length": 3044}
{"pred": "The source domain is where we have labeled data, and the target domain is where we want to apply the model but have no or very little labeled data.", "answers": ["Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen", "we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)"], "all_classes": null, "length": 5061}
{"pred": "They compare with state-of-the-art RNN models such as LSTM, GRU, and Transformer models.", "answers": ["Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM"], "all_classes": null, "length": 3319}
{"pred": "NeuronBlocks includes a suite of neural network modules, including embedding layers, RNNs, CNNs, attention mechanisms, and various regularization techniques.", "answers": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "all_classes": null, "length": 1678}
{"pred": "They used the multilingual pronunciation dataset and the multilingual text dataset for their experiments.", "answers": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "all_classes": null, "length": 3244}
{"pred": "The baselines were not specified in the article.", "answers": ["varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"], "all_classes": null, "length": 2215}
{"pred": "They use English, Spanish, and Finnish in their experiments.", "answers": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "all_classes": null, "length": 4086}
{"pred": "They test their method on tasks such as Named Entity Recognition, Text Classification, and Sentiment Analysis.", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "Yes. They initialize the embeddings of the top 20,000 words with 300-dimensional GloVe embeddings.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": "Yes, the Polyglot system was used as a baseline in the evaluations.", "answers": ["No", "No"], "all_classes": null, "length": 2738}
{"pred": "They generate maps of industries, linguistic and semantic word classes, and psycholinguistic and semantic maps using the blog data. They also use the Meaning Extraction and Reporting Tool (MEMRITT) to measure usage of values and themes in the data.", "answers": ["using the Meaning Extraction Method", "Unanswerable"], "all_classes": null, "length": 1440}
{"pred": "The ML methods aim to identify argument components such as premises, conclusions, and argument structures.", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": "Ngrams of any length are aligned using PARENT.", "answers": ["Unanswerable", "Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4"], "all_classes": null, "length": 3827}
{"pred": "The Twitter dataset contains 1,800 conversation threads, roughly 14,000 tweets.", "answers": ["1,873 Twitter conversation threads, roughly 14k tweets", "1,873 Twitter conversation threads, roughly 14k tweets"], "all_classes": null, "length": 3721}
{"pred": "The 12 languages covered are not explicitly stated in the article, but they are mentioned that they include a mixture of major and under-resourced languages.", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
{"pred": "The model is applied to two datasets: an expanded version of the `Conversations for Deriving Personalized Wikipedia Recommendations' dataset and a dataset constructed from the ChangeLog of the Wikipedia page for 'Forecasting Wikipedia Pageviews'.", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": "No.", "answers": ["No", "No"], "all_classes": null, "length": 2276}
{"pred": "The quality of the data is evaluated through various checks including sentence-level BLEU score, perplexity, and manual inspection.", "answers": ["Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations"], "all_classes": null, "length": 2435}
{"pred": "They concatenate the final hidden states of the audio and text RNNs and pass them through a feed-forward neural network.", "answers": ["combines the information from these sources using a feed-forward neural model", "encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model"], "all_classes": null, "length": 3201}
{"pred": "The model improved by 2.11 BLEU points, 1.7 Flesch-Kincaid Grade Level, and 1.04 simplified Meas. on the WikiText-103 dataset. It also improved by 1.3 points on the BLEU metric on the Multi30k dataset.", "answers": ["For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\nFor the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.", "6.37 BLEU"], "all_classes": null, "length": 2271}
{"pred": "The results of a human evaluation on random 700 examples were reported.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3711}
{"pred": "Tweets going viral are those retweeted more than 1000 times.", "answers": ["Viral tweets are the ones that are retweeted more than 1000 times", "those that contain a high number of retweets"], "all_classes": null, "length": 3144}
{"pred": "It is unanswerable based on the given information. The article discusses the use of different neural architectures (CNN, LSTM, BERT) in propaganda detection, but it does not provide information on which one performs best by itself.", "answers": ["BERT"], "all_classes": null, "length": 1507}
{"pred": "The data was collected using crowdsourcing.", "answers": ["Android application"], "all_classes": null, "length": 3795}
{"pred": "The article mentions Logistic Regression, Naive Bayes, Support Vector Machines, and Neural Networks for question similarity and Logistic Regression, Naive Bayes, and Support Vector Machines for question matching in the context of RQE. However, the specific machine learning models used for RQE in the study are Logistic Regression and Neural Networks.", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": "The benchmark dataset is a dataset used for building and testing machine learning models. Its quality is extensively explored in the article.", "answers": ["Social Honeypot dataset (public) and Weibo dataset (self-collected); yes", "Social Honeypot, which is not of high quality"], "all_classes": null, "length": 2242}
{"pred": "The decoder is an LSTM decoder.", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "The article does not specify whether the results are reported only on English data or not.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3838}
{"pred": "The ensemble of models (r19 and r21) in the article is ranked 3rd in the Fragment Sentence Classification task (FLC) for the Fragment Sentence Classification shared task at ACL 2020, with a score of 0.6511 for F1-score and 0.6521 for accuracy.", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "The baseline was a weak NMT model without any fine-tuning or additional resources.", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": "The highest recall score was 0.7033.", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": "Word embedding techniques such as word2vec and GloVe are explored in the paper.", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": "They use a bilingual dictionary or a translation system to match words before reordering them.", "answers": ["Unanswerable", "CFILT-preorder system"], "all_classes": null, "length": 2231}
{"pred": "Yes. The paper discusses methods for extracting medical information from electronic health records using BioIE techniques.", "answers": ["Yes"], "all_classes": null, "length": 3035}
{"pred": "Seven experts with legal training were used for annotation.", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "The model for generating poems from images is a CNN-LSTM model, and for text style transfer, a sequence-to-sequence model with attention is used.", "answers": ["generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models"], "all_classes": null, "length": 1653}
{"pred": "The transformer layer works better on top of BERT. (ToBERT outperforms RoBERT in the article.)", "answers": ["Transformer over BERT (ToBERT)", "The transformer layer"], "all_classes": null, "length": 2655}
{"pred": "Yes, the authors hypothesize that humans' robustness to noise is due to their general knowledge.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": "They addressed cyberbullying related to personal attacks, racism, and sexism.", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "They propose a new context representation for CNNs in relation classification.", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "The article mentions three major types of entities: Person (PER), Location (LOC), and Organization (ORG). However, it also mentions an additional category called MISC. Therefore, the total number of entity types is four.", "answers": ["OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities", "three"], "all_classes": null, "length": 2851}
{"pred": "The article shows that expert annotations are of higher quality than crowd-sourced annotations, but it does not provide a quantitative measure of the difference in quality.", "answers": ["improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added"], "all_classes": null, "length": 4399}
{"pred": "Women represent only 25.89% of the speech in the analyzed corpora.", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": "The approach achieves state of the art results on the English-German translation task using the Multi-Genre Multi-Domain English-German Parallel Corpus (MMC) dataset.", "answers": ["the English-German dataset"], "all_classes": null, "length": 1833}
{"pred": "Strong baselines are compared to other CWS models.", "answers": ["Baseline models are:\n- Chen et al., 2015a\n- Chen et al., 2015b\n- Liu et al., 2016\n- Cai and Zhao, 2016\n- Cai et al., 2017\n- Zhou et al., 2017\n- Ma et al., 2018\n- Wang et al., 2019"], "all_classes": null, "length": 3629}
{"pred": "Logistic Regression and Multilayer Perceptron (MLP) are used as classifiers.", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": "They use BIBTEX entries for NLTK, Stanford CoreNLP, Twitter's Text API, NLTK's Vader Sentiment Analysis, and spaCy.", "answers": ["BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21", "BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26"], "all_classes": null, "length": 1452}
{"pred": "Experiments are conducted on the SQuAD dataset.", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "Various approaches have been proposed for modelling urban environments using Flickr photos, including modelling urban regions, identifying points of interest, and personalizing recommendations. However, the use of Flickr photos for characterizing natural environments has received less attention. The article discusses the potential of using Flickr photos to model natural environments and proposes a method for doing so.", "answers": ["BOW-Tags, BOW-KL(Tags), BOW-All, GloVe"], "all_classes": null, "length": 4658}
{"pred": "Yes. They use the attention function to compute the similarity between the question and the passage.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1687}
{"pred": "They used the CSAT dataset for CSAT prediction, CSAT-ASR dataset for CSAT prediction from ASR transcripts, and the CSAT-ASR-Senti dataset for CSAT prediction from ASR transcripts with sentiment analysis.", "answers": ["CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus", "CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus"], "all_classes": null, "length": 2652}
{"pred": "The IMDb movie review dataset is used for sentiment classification.", "answers": ["the IMDb movie review dataset BIBREF17", "IMDb movie review"], "all_classes": null, "length": 3432}
{"pred": "Yes, the tasks were evaluated in previous work, but the evaluation was done using unidirectional language models.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1464}
{"pred": "In future work, the plan is to use a balanced dataset for sentiment analysis.", "answers": ["No"], "all_classes": null, "length": 1441}
{"pred": "The invertibility condition is a requirement for the neural network to have an inverse and a Jacobian determinant that is always non-zero.", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": "The proposed qualitative annotation schema includes linguistic and factual annotations, as well as annotations related to the reasoning process and the external context. It is used to evaluate the performance of machine learning models in understanding and generating text.", "answers": ["The resulting taxonomy of the framework is shown in Figure FIGREF10", "FIGREF10"], "all_classes": null, "length": 4958}
{"pred": "WikiLarge includes 2,000 sentence pairs for testing and 297,000 sentence pairs for training. WikiSmall includes 10,000 sentence pairs for testing and 19,000 sentence pairs for training.", "answers": ["training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing", "WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. "], "all_classes": null, "length": 2266}
{"pred": "The article compares the proposed method with several baselines, including a vanilla ST model, pre-trained models, and models with different attention mechanisms.", "answers": ["Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation", "Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train", "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\n\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\n\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. "], "all_classes": null, "length": 4704}
{"pred": "English\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: The main contribution of this paper is the incorporation of cost-sensitive learning into BERT for fine-grained sentiment analysis.\n\nQuestion: What is the dataset used in this paper?\n\nAnswer: The dataset used in this paper is the SemEval-2017 Task 4 dataset for fine-grained sentiment analysis.\n\nQuestion: What is the performance of the proposed method on the dataset?\n\nAnswer: The proposed method achieves state-of-the-art performance on the Sem", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 4513}
{"pred": "The models used in the experiment are a linear SVM, a bidirectional LSTM, and a CNN.", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "Unanswerable. The article focuses on the linguistic and structural differences between open and answered questions, but it does not mention anything about the measurement or evaluation of answer usefulness.", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "200-dimensional GloVe embeddings trained on Twitter data, 200-dimensional Edinburgh embeddings, and 200-dimensional EmojiNet embeddings were used. Additionally, 200-dimensional EmoInt embeddings were created specifically for this task.", "answers": ["Pretrained word embeddings  were not used", "GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16"], "all_classes": null, "length": 1771}
{"pred": "They released a new dataset of 180K recipes and 750K user reviews for recipe recommendation. They also introduced new evaluation strategies for generation quality. Their personalized models outperformed the baselines in terms of both user satisfaction and engagement.", "answers": ["average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time"], "all_classes": null, "length": 2666}
{"pred": "The combination of rewards for reinforcement learning is not explicitly stated in the article. However, it is mentioned that the model utilizes an irony reward, an irony reward with sentiment analysis, and a sentiment reward for sentiment analysis. It is also mentioned that the model uses a sentiment reward for sentiment analysis and a sentiment reward with attention mechanism for sentiment analysis. Therefore, it can be inferred that the combination of rewards for reinforcement learning includes at least these rewards. However, the exact combination and how they are combined is not specified in the article.", "answers": ["irony accuracy, sentiment preservation", " irony accuracy and sentiment preservation"], "all_classes": null, "length": 4592}
{"pred": "They demonstrate that the model may not work well when the style transfer dataset does not have similar poems to the given English poem.", "answers": ["Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer", "we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score"], "all_classes": null, "length": 1651}
{"pred": "They compared their models to existing ones on the Affective Text Reaction Corpus, the Emotion Intensity Dataset, and the Emotion Intensity Annotated Corpus.", "answers": ["Affective Text, Fairy Tales, ISEAR", " Affective Text dataset, Fairy Tales dataset, ISEAR dataset"], "all_classes": null, "length": 3390}
{"pred": "The article presents the distribution results of various attributes for viral tweets containing fake news and those containing real news.", "answers": ["Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different"], "all_classes": null, "length": 3164}
{"pred": "The dataset includes 12,548 unique English hashtags and their associated tweets from the Stanford Sentiment Treebank and the SemEval-2017 Task 4 dataset.", "answers": ["1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset", "Stanford Sentiment Analysis Dataset BIBREF36"], "all_classes": null, "length": 3756}
{"pred": "The article does not provide information on the accents present in the corpus.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "Word subspace is a compact, scalable, and meaningful representation of a set of word vectors, which can effectively capture the semantic relationships between words.", "answers": ["Word vectors, usually in the context of others within the same class"], "all_classes": null, "length": 5151}
{"pred": "The baseline models used are Random Forest and Naive Bayes.", "answers": ["For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. \n\nFor Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.", "B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .\n\n, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0"], "all_classes": null, "length": 7891}
{"pred": "SemCor3.0 is used as the training corpus in the article, not SemCor3.0. Therefore, it is unanswerable whether SemCor3.0 is reflective of English language data in general.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2000}
{"pred": "The article mentions a 110-hour Augmented LibriSpeech dataset, but it does not provide the size in hours for the other corpora mentioned. Therefore, it is unanswerable to determine the size of Augmented LibriSpeech based on the given article alone.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2410}
{"pred": "They used the datasets for fine-grained sentiment analysis released by the SemEval-2017 Task 4 challenge.", "answers": [" high-quality datasets  from SemEval-2016 “Sentiment Analysis in Twitter” task", " SemEval-2016 “Sentiment Analysis in Twitter”"], "all_classes": null, "length": 2738}
{"pred": "They use the pre-trained uncased BERTBASE model.", "answers": ["small BERT", "small BERT"], "all_classes": null, "length": 1999}
{"pred": "Yes, the article discusses the importance of carefully constructing and validating automatically generated datasets to ensure their quality.", "answers": ["No", "No"], "all_classes": null, "length": 6391}
{"pred": "The article describes an image captioning task, but it does not specify the domain of the images.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3472}
{"pred": "The article reports competitive or state-of-the-art results for emotion detection using Facebook data and SVM models.", "answers": ["Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. "], "all_classes": null, "length": 3410}
{"pred": "The article proposes a new tagging scheme consisting of three tags: INLINE\\_START, INLINE\\_END, and PUNCT.", "answers": ["A new tagging scheme that tags the words before and after the pun as well as the pun words.", "a new tagging scheme consisting of three tags, namely { INLINEFORM0 }"], "all_classes": null, "length": 2974}
{"pred": "No.", "answers": ["No", "No"], "all_classes": null, "length": 2413}
{"pred": "The article investigates the robustness of a model to prior knowledge and prior assumptions in text classification. The authors propose methods to make the model more robust by incorporating prior knowledge in a flexible way and by using regularization techniques. They also discuss the importance of evaluating the robustness of a model and propose a framework for doing so.\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: The main contribution of the paper is the development of a new method for text classification that incorporates prior knowledge in a flexible way and uses regularization techniques to make the model more robust to prior assumptions. The authors", "answers": ["ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced", "Low sensitivity to bias in prior knowledge"], "all_classes": null, "length": 3609}
{"pred": "InferSent, BERT, RoBERTa, and Sentence-BERT are evaluated.", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "The method achieves F1 improvements of +0.9 for English datasets and +2.3 for Chinese datasets for NER task.", "answers": ["English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively", "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively"], "all_classes": null, "length": 3586}
{"pred": "They test their conflict method on Task 1: Quora Duplicate Question Detection and Task 2: Ranking.", "answers": ["Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions", "Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask"], "all_classes": null, "length": 2577}
{"pred": "They compared against various syntactic tree models, sequential models, and other neural network models.", "answers": ["Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks", "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."], "all_classes": null, "length": 4781}
{"pred": "The core component for KBQA is relation detection and linking.", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "The baseline models are a name-based Nearest Neighbor model and an Encoder-Decoder model with ingredient attention.", "answers": ["name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)"], "all_classes": null, "length": 2655}
{"pred": "The article discusses methods such as manual annotation, tagging, and part-of-speech tagging to detect biases and unwarranted inferences in the Flickr30k dataset. It also suggests using tools like coreference resolution and named entity recognition to identify stereotypical or unwarranted assumptions. Additionally, the author proposes using part-of-speech tagging and dependency parsing to identify linguistic patterns that may indicate biases or unwarranted inferences.", "answers": ["spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering", "Looking for adjectives marking the noun \"baby\" and also looking for most-common adjectives related to certain nouns using POS-tagging"], "all_classes": null, "length": 2204}
{"pred": "Winograd Schema Challenge is a test for AI programs.", "answers": ["English, French, German ", "French, English, Spanish, Italian, Portuguese, Hebrew, Arabic"], "all_classes": null, "length": 2285}
{"pred": "They experimented with a bidirectional LSTM model and a transformer model.", "answers": ["Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers"], "all_classes": null, "length": 3210}
{"pred": "No, they compare their results with other methods on a relatively small semantic similarity dataset in multiple languages.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 6169}
{"pred": "The authors explored a few summarization algorithms provided by the Sumy package.", "answers": ["LSA, TextRank, LexRank and ILP-based summary.", "LSA, TextRank, LexRank"], "all_classes": null, "length": 3045}
{"pred": "The previous state of the art for this task was probabilistic models and rule-based systems. (BibTeX entries BIBREF0 and BIBREF1 in the article refer to these works.)", "answers": ["hLSTM", "hLSTM"], "all_classes": null, "length": 3725}
{"pred": "It is unanswerable based on the provided article.", "answers": ["Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets."], "all_classes": null, "length": 4180}
{"pred": "The corpora used in the shared task correspond to the diachronic corpus pair DTA18 and DTA19 from the DTA corpus.", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "Kannada, Hindi, Telugu, Tamil, Bengali, Malayalam, and Punjabi. (Note: The article mentions 7 Indian languages but does not explicitly state that Kannada is one of them. However, based on the context, it can be inferred that Kannada is one of the languages used in the experiments.)", "answers": ["Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam", "Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)"], "all_classes": null, "length": 2453}
{"pred": "The article explores the performance of a zero-shot transfer learning model on target language reading comprehension using multi-lingual datasets and fine-tuning. The results show that the model achieves reasonable performance on target languages, even without fine-tuning, but fine-tuning significantly improves the results. The authors also find that the model's performance on target languages is comparable to that on the source language, English. Therefore, the answer is: The model achieves reasonable performance on target language reading comprehension.", "answers": ["Table TABREF6, Table TABREF8", "when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En"], "all_classes": null, "length": 2492}
{"pred": "The article reports that the proposed model, ALOHA, outperforms the baselines in terms of Hits@1, Hits@3, and Hits@5 metrics. The exact difference in performance is not specified in the article.", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": "The article does not provide specific numbers on the improvement gained from Adversarial Reward Augmented Maximum Likelihood (ARAML).", "answers": ["ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.", "Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively."], "all_classes": null, "length": 3796}
{"pred": "The authors present evidence of mislabeled items in the datasets and suggest that these mislabelings may be due to biases in data collection and annotation. They also mention that their model is able to detect some of these biases.", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": "Yes, other baselines were tested to compare with the neural baseline.", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "The dataset is divided into three parts with 20% for development, 60% for training, and 20% for testing. The total number of entities in the dataset is 10,582.", "answers": ["Dataset contains 3606 total sentences and 79087 total entities.", "ILPRL contains 548 sentences, OurNepali contains 3606 sentences"], "all_classes": null, "length": 2843}
{"pred": "The proposed method improves F1 score by +0.54 for MRPC and +0.73 for QQP.", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "The ERP data from the BES study and the EEG data from the MMN study are used.", "answers": ["Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.\nSelect:\n- ERP data collected and computed by Frank et al. (2015)\n- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)", "the ERP data: BIBREF0"], "all_classes": null, "length": 1971}
{"pred": "The subjects were presented with speech stimuli to elicit event-related responses.", "answers": ["7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", "KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)"], "all_classes": null, "length": 2379}
{"pred": "Pointer to the section describing the baselines: \"We experiment and compare with the following models: Pointer to the list of baselines: Pointer to the section describing the evaluation metrics: \"We evaluate the performance of our model using the following metrics: ROUGE-1, ROUGE-2, and ROUGE-L for automatic evaluation, and human evaluation for manual evaluation.\"", "answers": ["Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN"], "all_classes": null, "length": 4085}
{"pred": "The paper investigates the efficacy of different learning models, including Naive Bayes, Support Vector Machines, Long Short-Term Memory (LSTM), and Convolutional Neural Networks (CNN), on the dataset.", "answers": ["Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)", "Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN"], "all_classes": null, "length": 2074}
{"pred": "Both a bi-directional language model and a uni-directional language model are used.", "answers": ["uni-directional model to augment the decoder", "bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder"], "all_classes": null, "length": 1914}
{"pred": "The weights are dynamically adjusted based on the probability of the example being easy or hard during training. The weight is multiplied by $(1-p)^{γ}$, where $p$ is the probability of the example being easy and $γ$ is a hyperparameter. This strategy helps the model focus more on hard examples and reduces the influence of easy examples.", "answers": ["One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.", "associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds"], "all_classes": null, "length": 3640}
{"pred": "The article shows that agents utilizing knowledge of knowledge graphs and enhanced exploration strategies significantly outperform baseline methods in text-based RPG games.", "answers": ["Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.", "KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40"], "all_classes": null, "length": 2443}
{"pred": "An individual model in the context of the article consists of a Bayesian probabilistic model for each language in the multilingual system.", "answers": ["Bayesian model of garg2012unsupervised as our base monolingual model"], "all_classes": null, "length": 3701}
{"pred": "Non-standard pronunciations are identified through annotations in the transcription.", "answers": ["Unanswerable", "Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation."], "all_classes": null, "length": 3018}
{"pred": "A semicharacter architecture is a type of neural network model for word recognition that processes words as sequences of characters, but treats the first and last character differently from the internal characters. It was proposed in [BIB07].", "answers": ["A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters", "processes a sentence of words with misspelled characters, predicting the correct words at each step"], "all_classes": null, "length": 4186}
{"pred": "The article compares the performance of feature-based and deep learning models on PoS tagging for 16 languages: Bulgarian, Czech, Danish, Dutch, English, German, Italian, Norwegian, Polish, Portuguese, Romanian, Russian, Slovak, Spanish, and Swedish.", "answers": ["Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish", "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish"], "all_classes": null, "length": 2697}
{"pred": "The article reports that NCEL outperforms various baselines and state-of-the-art methods in entity linking and relation extraction tasks.", "answers": ["NCEL consistently outperforms various baselines with a favorable generalization ability"], "all_classes": null, "length": 4113}
{"pred": "Yes. The data is de-identified by removing identifying information and replacing it with digital zeros and [de-identified] tags.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": "The baseline was the performance of the error detection system using only the original training data.", "answers": ["error detection system by Rei2016", "error detection system by Rei2016"], "all_classes": null, "length": 2132}
{"pred": "They obtained the annotated clinical notes from the CE2010 dataset in the 2010 i2b2 NLP challenge.", "answers": ["clinical notes from the CE task in 2010 i2b2/VA", "clinical notes from the CE task in 2010 i2b2/VA "], "all_classes": null, "length": 3432}
{"pred": "Masking words in the decoder and feeding them to BERT for contextualized representation helps the model generate more accurate and diverse outputs. (From the article, \"At each time step, the decoder predicts the next word based on the previous words and the context vector from the summary. The context vector is obtained by feeding the previous words and the current summary to BERT and taking the last hidden state as the context vector. We mask each word in the summary one by one and feed the masked summary to BERT to obtain the context vector for the masked word. The decoder then predicts the masked word", "answers": ["ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences."], "all_classes": null, "length": 3919}
{"pred": "The article does not mention which dataset is used.", "answers": ["Unanswerable", " Paraphrase Database (PPDB) ,  book corpus", "Unanswerable"], "all_classes": null, "length": 1902}
{"pred": "TF-IDF features are used.", "answers": ["Unanswerable"], "all_classes": null, "length": 2108}
{"pred": "The dataset is annotated with depressive symptoms and their severity.", "answers": ["no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy", "The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression"], "all_classes": null, "length": 1947}
{"pred": "They evaluated on eight publicly available Named Entity Recognition (NER) tasks.", "answers": ["BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800", "BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800"], "all_classes": null, "length": 2800}
{"pred": "The machine translation platform Apertium and the machine translation platform Apertium were used to translate the training data into Spanish.", "answers": ["using the machine translation platform Apertium ", "machine translation platform Apertium BIBREF5"], "all_classes": null, "length": 2423}
{"pred": "They used a multinomial naive Bayes classifier for their system.", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": "The baseline for the Sentence-level task was a simple logistic regression classifier with bag-of-words features. For the Document-level task, the baseline was a simple Naive Bayes classifier with bag-of-words features.", "answers": ["The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.", "SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly"], "all_classes": null, "length": 3001}
{"pred": "They compare with baselines that do not adopt joint learning and joint decoding, such as conditional random fields and recurrent neural networks.", "answers": ["They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF."], "all_classes": null, "length": 2991}
{"pred": "The political bias of different sources is included in the model by assigning a political bias label to each source and news article, following the procedure described in [BIBLIOGRAPHY].", "answers": ["By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains", "we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries."], "all_classes": null, "length": 4882}
{"pred": "The ancient Chinese dataset is collected from the internet, primarily from ancient Chinese history records and articles written by celebrities during certain dynasties.", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "English.", "answers": ["English", "English ", "English"], "all_classes": null, "length": 2240}
{"pred": "The article mentions that the proposed approach performs favorably on standard English and Chinese datasets. However, it does not specify which Chinese datasets were used.", "answers": ["Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)"], "all_classes": null, "length": 2545}
{"pred": "The UTCNN model has three convolutional layers and two fully connected layers.", "answers": ["eight layers"], "all_classes": null, "length": 4487}
{"pred": "The dataset used in this paper is not explicitly stated in the provided text. However, it can be inferred that the authors used a set of 70 million geotagged Flickr photos and 11 million geotagged Instagram photos for their study.", "answers": [" the same datasets as BIBREF7", "same datasets as BIBREF7"], "all_classes": null, "length": 4661}
{"pred": "The paper uses two clinical datasets, NUBIA's MEDIC and NLP for Clinical Text Analysis (NLPCTA), for the experiments.", "answers": ["MEDDOCAN, NUBes-PHI", "MEDDOCAN, NUBes "], "all_classes": null, "length": 4528}
{"pred": "They used unigrams, pragmatic patterns, and stylistic features.", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "The article reports experimental results showing that LiLi outperforms other chatbots in terms of predictive accuracy and conversational ability. However, it does not explicitly state which metrics were used to evaluate these improvements.", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": "Yes, they use Lucene indexing to create a sample dataset for their experiments.", "answers": ["Yes", "No"], "all_classes": null, "length": 1910}
{"pred": "Galatasaray (Target-1) and Fenerbahçe (Target-2)", "answers": ["Galatasaray, Fenerbahçe", "Galatasaray , Fenerbahçe "], "all_classes": null, "length": 2234}
{"pred": "Automatic evaluations and human evaluations are conducted on the model's performance in generating ironic tweets.", "answers": ["Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences"], "all_classes": null, "length": 4600}
{"pred": "Gaussian masked attention pays attention to adjacent characters in a sequence, while multi-head attention attends to different aspects of the input. Gaussian masked multi-head attention combines both approaches by attending to different directions in a sequence.", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "They considered Facebook and Twitter.", "answers": ["Facebook status update messages", "Facebook status update messages"], "all_classes": null, "length": 4005}
{"pred": "The network's baseline features are extracted from the fully connected layer of the baseline CNN model.", "answers": [" The features extracted from CNN."], "all_classes": null, "length": 4855}
{"pred": "The number of clusters and the number of iterations for k-means clustering were varied in the experiments on the four tasks.", "answers": ["number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding", "different number of clusters, different embeddings"], "all_classes": null, "length": 2753}
{"pred": "Their system ranked second, fourth, and fifth on the development set for EI-Valence, EI-Arousal, and EI-Dominance tasks, respectively. Their official scores on the test set placed them second for EI-Valence, fourth for EI-Arousal, and fifth for EI-Dominance.", "answers": ["column Ens Test in Table TABREF19"], "all_classes": null, "length": 2424}
{"pred": "The corpus consists of 8,167 sentences and 163,140 tokens.", "answers": ["8,275 sentences and 167,739 words in total", "The corpus comprises 8,275 sentences and 167,739 words in total."], "all_classes": null, "length": 2669}
{"pred": "Yes, the article discusses a method for generating cloze-style questions from a document and then using these questions for pretraining a QA model. However, it does not explicitly mention converting cloze-style questions to naturally-looking questions.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2764}
{"pred": "They consider text categorization and sentiment classification.", "answers": ["text classification for themes including sentiment, web-page, science, medical and healthcare"], "all_classes": null, "length": 3591}
{"pred": "Their model is compared to five previous methods, including rules-based methods, dependency parsing, and machine learning models.", "answers": ["bag-of-words model, CNN"], "all_classes": null, "length": 5838}
{"pred": "The newer versions of ELMo were trained on larger datasets.", "answers": ["By 14 times.", "up to 1.95 times larger"], "all_classes": null, "length": 3294}
{"pred": "The dataset contains sentences. (The exact number is not mentioned in the article.)", "answers": ["3606", "6946"], "all_classes": null, "length": 2835}
{"pred": "They compare the proposed s2sLSTM model to LSTM and GRU models.", "answers": ["MLP", "Eusboost, MWMOTE"], "all_classes": null, "length": 2474}
{"pred": "Yes, their NER model takes both text and image as input.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3784}
{"pred": "Yes.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4320}
{"pred": "The highest MRR score was 0.611.", "answers": ["0.5115", "0.6103"], "all_classes": null, "length": 6810}
{"pred": "They evaluate on the Wall Street Journal portion of the Penn Treebank.", "answers": [" Wall Street Journal (WSJ) portion of the Penn Treebank", "Unanswerable"], "all_classes": null, "length": 4327}
{"pred": "The authors conducted a survey among engineers and identified that about 80% of them faced challenges when applying NLP models to specific tasks. They also analyzed the most common NLP jobs and found that more than 90% of them used a few common components. This suggests that engineers spend a significant amount of time choosing from various frameworks, models, and optimization techniques.", "answers": ["By conducting a survey among engineers", "Unanswerable"], "all_classes": null, "length": 1692}
{"pred": "They achieve state-of-the-art results on both SimpleQuestions and WebQuestions benchmarks.", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
